# seq, gnn, fbnetgen
name: BrainNetworkTransformer
sizes: [360, 100]
pooling: [false, true]
pos_encoding: none
orthogonal: true
freeze_center: true
project_assignment: true
pos_embed_dim: 360


model:
encoder: modular         # ä»ç„¶å« modularï¼›ä¹Ÿå¯è‡ªå®šä¹‰åå­—
fuse: gate               # gate | concat
gate:
  granularity: node      # global | node
  hidden: 1              # 0=çº¿æ€§é—¨æ§ï¼›>0=ä¸¤å±‚MLP
  dropout: 0.99
entmax:
  alpha: 1.2             # 1.2~1.5 å¸¸ç”¨
film:
  dim: 64
  # ä¸‹é¢ä¸¤ä¸ªä¼šè¢«é€ä¼ åˆ° TwoBranchEncoderLayerï¼Œç¡®ä¿ä¸åŸç‰ˆä¸€è‡´
dropout: 0.1
hidden_size: 1024        # è¿™ä¼šä½œä¸º dim_feedforward ä¼ ä¸‹å»
module_map_path: D:\code\BrainNetworkTransformer-main\source\conf\dataset\group_modules_k7_zero_based.npy

## ğŸ‘‰ è¿™äº›é”®ä¸ä¸Šé¢åŒçº§ï¼Œä¸è¦å†åŒ…ä¸€å±‚ model:
#encoder: modular           # vanilla | modular
## module_map_path: D:\code\BrainNetworkTransformer-main\source\conf\dataset\group_modules_k7.npy
#module_map_path: D:\code\BrainNetworkTransformer-main\source\conf\dataset\group_modules_k7_zero_based.npy
#fuse: sum_eq            # gate | sum | concat  sum_eq
#gate:
#  granularity: node  # global  node
#entmax:
#  alpha: 1.001
#film:
#  dim: 64
#  place: pre_qkv
#
#attn_dropout: 0.05
#ffn_dropout: 0.1
#gate_dropout: 0.0      # æˆ– 0.05ï¼›åˆ«ä¸€å¼€å§‹å°±å¤§